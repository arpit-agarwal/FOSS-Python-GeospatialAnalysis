{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating map from csv file \n",
    "\n",
    "1. data is [from](https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms/viirs-i-band-active-fire-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert a mulitpart geometry into seprate geometries, the geometry is after subject to mapshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "# from shapely.geometry.polygon import Polygon\n",
    "# from shapely.geometry.multipolygon import MultiPolygon\n",
    "\n",
    "# def explode(gdf):\n",
    "#     \"\"\" \n",
    "#     Explodes a geodataframe \n",
    "    \n",
    "#     Will explode muti-part geometries into single geometries. Original index is\n",
    "#     stored in column level_0 and zero-based count of geometries per multi-\n",
    "#     geometry is stored in level_1\n",
    "    \n",
    "#     Args:\n",
    "#         gdf (gpd.GeoDataFrame) : input geodataframe with multi-geometries\n",
    "        \n",
    "#     Returns:\n",
    "#         gdf (gpd.GeoDataFrame) : exploded geodataframe with a new index \n",
    "#                                  and two new columns: level_0 and level_1\n",
    "        \n",
    "#     \"\"\"\n",
    "#     gs = gdf.explode()\n",
    "#     gdf2 = gs.reset_index().rename(columns={0: 'geometry'})\n",
    "#     gdf_out = gdf2.merge(gdf.drop('geometry', axis=1), left_on='level_0', right_index=True)\n",
    "#     gdf_out = gdf_out.set_index(['level_0', 'level_1']).set_geometry('geometry')\n",
    "#     gdf_out.crs = gdf.crs\n",
    "#     return gdf_out\n",
    "\n",
    "indata='../data/vector/India_states.json'\n",
    "gdf=gpd.GeoDataFrame.from_file(indata)\n",
    "# qq=explode(gdf)\n",
    "gdf.to_file('India_states.shp', driver='ESRI Shapefile')\n",
    "#gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T20:16:58.863479Z",
     "start_time": "2018-09-27T20:14:51.335460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pysal/__init__.py:65: VisibleDeprecationWarning: PySAL's API will be changed on 2018-12-31. The last release made with this API is version 1.14.4. A preview of the next API version is provided in the `pysal` 2.0 prelease candidate. The API changes and a guide on how to change imports is provided at https://pysal.org/about\n",
      "  ), VisibleDeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 0/365 in 2017\n",
      "completed 1/365 in 2017\n",
      "completed 2/365 in 2017\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from numpy import linspace\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
    "from descartes import PolygonPatch\n",
    "import geopandas\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import LineString\n",
    "from pysal.esda.mapclassify import Natural_Breaks as nb\n",
    "from pysal.esda.mapclassify import User_Defined as ud\n",
    "import fiona\n",
    "import os\n",
    "from os.path import exists\n",
    "import sys\n",
    "import matplotlib.patheffects as pe\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "\n",
    "dba = pd.read_csv('../data/vector/fire_archive_V1_2871.csv')\n",
    "\n",
    "\n",
    "\n",
    "def lookup(s):\n",
    "    \"\"\"\n",
    "    This is an extremely fast approach to datetime parsing.\n",
    "    For large data, the same dates are often repeated. Rather than\n",
    "    re-parse these, we store all unique dates, parse them, and\n",
    "    use a lookup to convert all dates.\n",
    "    \"\"\"\n",
    "    return s.map({date:pd.to_datetime(date) for date in s.unique()})\n",
    "\n",
    "dba.loc[:,'date']=lookup(dba['acq_date'])\n",
    "\n",
    "dba['monthday'] = dba['date'].dt.strftime('%j')\n",
    "dba['fileformate'] = dba['date'].dt.strftime('%Y_%m_%d')\n",
    "dba['year'] = dba['date'].dt.strftime('%Y')\n",
    "\n",
    "\n",
    "yearlist=dba.drop_duplicates('year')\n",
    "yearlist1=yearlist['year'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "# choose dpi\n",
    "\n",
    "dpichos=300\n",
    "\n",
    "###########################\n",
    "#speicify the path location of the script file, in which folder the shape file and data csv file has to be placed\n",
    "\n",
    "path=\"./\"\n",
    "os.chdir(path)\n",
    "\n",
    "############################\n",
    "#location of the legend to show, choose legendpos=left or right, comment the line otherwise\n",
    "\n",
    "#left=fig.add_axes([0.12, 0.1, 0.04, 0.25]) \n",
    "\n",
    "#right=fig.add_axes([0.8, 0.1, 0.03, 0.3]) \n",
    "#[left,right,height, width]\n",
    "#legendpos=left\n",
    "\n",
    "############################\n",
    "#the background color, chnage the string 'w' to get different colors\n",
    "\n",
    "tbg='w'\n",
    "\n",
    "###########################\n",
    "#output file type, edit the outputtype into '.jpg' to get it in jpg  \n",
    "\n",
    "outputtype='.png'\n",
    "\n",
    "##########################\n",
    "# resolution/size, edit size into small to get the ouptut in small size \n",
    "\n",
    "India=[8.6,8.6]\n",
    "\n",
    "size=India\n",
    "\n",
    "##########################\n",
    "#get the exxtent of shape file, state shape file read and takes the extent values from it\n",
    "\n",
    "shp = fiona.open('India_states.shp')\n",
    "bds = shp.bounds\n",
    "shp.close()\n",
    "extra = 0.01\n",
    "ll = (bds[0], bds[1])\n",
    "ur = (bds[2], bds[3])\n",
    "coords = list(chain(ll, ur))\n",
    "w, h = coords[2] - coords[0], coords[3] - coords[1]\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "#does reading of shape file and make a pandas datframe and read csv file to attach it with shape file\n",
    "for year in yearlist1:\n",
    "    dba1=dba[dba['year']=='2017']\n",
    "    daylist=dba1.drop_duplicates('monthday')\n",
    "    daylist1=daylist['monthday'].tolist()\n",
    "    for indef,houre in enumerate(daylist1[0:12]):\n",
    "        df5e=dba1[dba1.monthday==houre]\n",
    "        df5e['daye']=pd.to_datetime(df5e['acq_date'], format='%Y-%m-%d')\n",
    "        df5e['day1'] = df5e['daye'].dt.strftime('%Y-%b-%d')\n",
    "        timet=df5e.iloc[-1]['day1']\n",
    "        date=df5e.iloc[0]['fileformate']\n",
    "        newfilename='viirs_'+date\n",
    "        fig = plt.figure()\n",
    "        statepa = fig.add_subplot(111, facecolor=tbg, frame_on=False)\n",
    "        amap = Basemap(epsg=24374,lon_0=19.17,lat_0=72.98,ellps = 'WGS84',llcrnrlon=coords[0],llcrnrlat=coords[1],urcrnrlon=coords[2]+0.8,urcrnrlat=coords[3],lat_ts=0,resolution='i',suppress_ticks=True, ax=statepa)\n",
    "        amap.readshapefile('India_states','India_states',color='none',zorder=2)\n",
    "        df_map = pd.DataFrame({'geometry': [Polygon(xy) for xy in amap.India_states],'censuscode': [state['censuscode'] for state in amap.India_states_info]})\n",
    "        df_map['patches'] = df_map['geometry'].map(lambda x: PolygonPatch(x,fc='#d7dac2',ec='#ffffff', lw=.85, alpha=.9,zorder=4))\n",
    "        ###READING THE FIRE DATA\n",
    "        map_points = pd.Series([Point(amap(mapped_x, mapped_y)) for mapped_x, mapped_y in zip(df5e['longitude'], df5e['latitude'])])\n",
    "        dev = amap.scatter([geom.x for geom in map_points],[geom.y for geom in map_points],1, marker='o', lw=0,facecolor='r', edgecolor='b',alpha=0.6, antialiased=True,label='Active fire locations', zorder=3)\n",
    "        statepa.add_collection(PatchCollection(df_map['patches'].values, match_original=True))\n",
    "        x1,y1=amap(85.0,31.0)\n",
    "        plt.text(x1, y1, 'Himalayas', fontsize=12,fontweight='bold',ha='left',va='center',color='k')\n",
    "        x1,y1=amap(67.0,16.0)\n",
    "        plt.text(x1, y1, 'Arabian Sea', fontsize=12,fontweight='bold',ha='left',va='center',color='k')\n",
    "        x1,y1=amap(85.2,13.0)\n",
    "        plt.text(x1, y1, 'Bay of Bengal', fontsize=12,fontweight='bold',ha='left',va='center',color='k')\n",
    "        x1,y1=amap(81.0,39.0)\n",
    "        plt.text(x1, y1, 'Open Fires Detected on '+timet, fontsize=15,fontweight='bold',ha='center',va='center',color='k')\n",
    "        citations='Source: NRT VIIRS 375 m \\nActive Fire product VNP14IMGT. \\nAvailable on-line [https://earthdata.nasa.gov/firms]. \\nDOI:10.5067/FIRMS/VIIRS/VNP14IMGT.NRT.001.'\n",
    "        x1,y1=amap(66.0,7.5)\n",
    "        plt.text(x1, y1, citations, fontsize=6,fontweight='bold',ha='left',va='center',color='k')\n",
    "        fig.set_size_inches(size[0],size[1])\n",
    "        plt.savefig(newfilename+outputtype, dpi=dpichos, alpha=True)\n",
    "        plt.close(fig)\n",
    "        print(\"completed \"+str(indef)+\"/365 in \"+str(year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dba.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stitching image into animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T20:20:12.654319Z",
     "start_time": "2018-09-27T20:19:26.551241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "import os\n",
    "import glob\n",
    "\n",
    "pngfiles=glob.glob('./*png')\n",
    "output_base_name='test.mp4'\n",
    "aa=[1]*len(pngfiles[0:2])\n",
    "clip = mpy.ImageSequenceClip(pngfiles[0:2], durations=aa, load_images=True)\n",
    "clip.write_videofile(output_base_name,audio=False,fps=24 )\n",
    "\n",
    "#pngfiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "from IPython.display import HTML\n",
    "<video width=\"700\" height=\"700\" controls>\n",
    "  <source src=\"test.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "31px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
